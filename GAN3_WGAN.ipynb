{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saquibali7/GANs/blob/main/GAN3_WGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b2cUJPmV9Pp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxstObIYWDAR"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, channels_img, features_d):\n",
        "    super(Discriminator, self).__init__()\n",
        "\n",
        "    self.disc = nn.Sequential(\n",
        "        nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        self._block(features_d, features_d*2, 4,2,1),\n",
        "        self._block(features_d*2, features_d*4, 4,2,1),\n",
        "        self._block(features_d*4, features_d*8, 4,2,1),\n",
        "        nn.Conv2d(features_d*8,1,kernel_size=4,stride=2,padding = 0),\n",
        "    )\n",
        "\n",
        "  def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "      return nn.Sequential(\n",
        "          nn.Conv2d(in_channels,out_channels, kernel_size, stride, padding, bias=False),\n",
        "          nn.InstanceNorm2d(out_channels, affine=True),\n",
        "          nn.LeakyReLU(0.2),\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "        return self.disc(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9iZ7QTSbHRF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, channels_noise, channels_img, features_g):\n",
        "    super(Generator, self).__init__()\n",
        "    self.gen = nn.Sequential(\n",
        "        self._block(channels_noise, features_g*16, 4, 1, 0),\n",
        "        self._block(features_g*16, features_g*8, 4,2,1),\n",
        "        self._block(features_g*8, features_g*4, 4,2,1),\n",
        "        self._block(features_g*4, features_g*2, 4,2,1),\n",
        "        nn.ConvTranspose2d(features_g*2, channels_img, kernel_size=4, stride=2, padding=1),\n",
        "        nn.Tanh(),\n",
        "    )\n",
        "\n",
        "  def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "      return nn.Sequential(\n",
        "          nn.ConvTranspose2d(\n",
        "                    in_channels, \n",
        "                    out_channels, \n",
        "                    kernel_size, \n",
        "                    stride, \n",
        "                    padding, \n",
        "                    bias=False),\n",
        "          nn.BatchNorm2d(out_channels), ## for WGAN with gradient penalty use nn.InstanceNorm2d\n",
        "          nn.ReLU(),          \n",
        "                  \n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "        return self.gen(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEgcEqjTbJ6_"
      },
      "outputs": [],
      "source": [
        "def initialise_weights(model):\n",
        "  for m in model.modules():\n",
        "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "      nn.init.normal(m.weight.data, 0.0, 0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GP0sD2SMbNWd"
      },
      "outputs": [],
      "source": [
        "def test():\n",
        "  N, in_channels, H, W = 8, 3, 64, 64\n",
        "  noise_dim = 100\n",
        "  x = torch.randn(N, in_channels, H, W)\n",
        "  disc = Discriminator(in_channels, 8)\n",
        "  initialise_weights(disc)\n",
        "  assert disc(x).shape == (N,1,1,1), \"fail\"\n",
        "  gen = Generator(noise_dim, in_channels, 8)\n",
        "  initialise_weights(gen)\n",
        "  z = torch.randn(N, noise_dim, 1, 1)\n",
        "  assert gen(z).shape == (N, in_channels, H, W), \"fail\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECiXa6NDbPBA",
        "outputId": "e38ae305-00d9-4e89-9085-c5e6ad9404f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ],
      "source": [
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98Pqx4yRbP30"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "LEARNING_RATE= 5e-5 ## form WGAN with gradient penalty lr = 1e-4\n",
        "BATCH_SIZE = 64\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS_IMG = 3\n",
        "NOISE_DIM = 128\n",
        "EPOCHS = 3\n",
        "FEATURES_DISC = 64\n",
        "FEATURES_GEN = 64\n",
        "CRITIC_ITERATIONS = 5\n",
        "WEIGHTS_CLIP = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_S90hl8gbxzL"
      },
      "outputs": [],
      "source": [
        "transforms = transforms.Compose(\n",
        "    [\n",
        "     transforms.Resize(IMAGE_SIZE),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize(\n",
        "         [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
        "     ),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Plh5HsOB-nEf",
        "outputId": "af719cac-636d-448a-ac2b-43126cdc32c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRU5eAap-s5C",
        "outputId": "ee8e77fa-0024-43c7-af6f-5763d628c46a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1478\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "path = '/content/drive/MyDrive/Morula/'\n",
        "file = os.listdir(path)\n",
        "print(len(file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pWjZqPc-3Ia"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vkolr2Go-5OQ"
      },
      "outputs": [],
      "source": [
        "train_transforms = A.Compose(\n",
        "    [   \n",
        "        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
        "        A.Normalize(\n",
        "         [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
        "        ),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6YloVqk_Mp7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.datasets as datasets\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHec7SK4_Jjl"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "  def __init__(self, root_dir, transform=None):\n",
        "    self.root_dir = root_dir\n",
        "    self.files = os.listdir(root_dir)\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.files)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_file = self.files[idx]\n",
        "    img_path = os.path.join(self.root_dir, img_file)\n",
        "    img = np.array(Image.open(img_path))\n",
        "    if self.transform:\n",
        "      img = self.transform(image=img)['image']\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swSA6B7nbx-3"
      },
      "outputs": [],
      "source": [
        "dataset = ImageDataset(root_dir=path, transform=train_transforms)\n",
        "loader = DataLoader(dataset, batch_size = BATCH_SIZE)\n",
        "\n",
        "gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
        "critic = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3xeYGQXb19U",
        "outputId": "a29e2696-0de6-4a73-8e0d-1f98d32a2635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ],
      "source": [
        "initialise_weights(critic)\n",
        "initialise_weights(gen)\n",
        "opt_critic = optim.RMSprop(critic.parameters(), lr=LEARNING_RATE)\n",
        "opt_gen = optim.RMSprop(gen.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUHbmY3-cIwC"
      },
      "outputs": [],
      "source": [
        "fixed_noise = torch.randn(32, NOISE_DIM, 1,1).to(device)\n",
        "writer_fake = SummaryWriter(f\"logs/fake\")\n",
        "writer_real = SummaryWriter(f\"logs/real\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a99Lo5Gr_q5i",
        "outputId": "73571eed-5c1e-4190-bbdf-17ff7864b7f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.1-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 30.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 60.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 70.6 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 77.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 79.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=f4bae658a93d3079c18520576e290e5493e6994f61535855f393030a8fbf62b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.1 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.1\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_penalty(critic, real, fake, device=\"cpu\"):\n",
        "    BATCH_SIZE, C, H, W = real.shape\n",
        "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
        "    interpolated_images = real * alpha + fake * (1 - alpha)\n",
        "\n",
        "    mixed_scores = critic(interpolated_images)\n",
        "\n",
        "    gradient = torch.autograd.grad(\n",
        "        inputs=interpolated_images,\n",
        "        outputs=mixed_scores,\n",
        "        grad_outputs=torch.ones_like(mixed_scores),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    gradient = gradient.view(gradient.shape[0], -1)\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
        "    return gradient_penalty\n",
        "\n"
      ],
      "metadata": {
        "id": "2PScvyQdFSrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7JYkJ8yArKj"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import make_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9ZmFtQmcOAl",
        "outputId": "e1e77d7b-5d1b-4e29-b415-01c5be2dc603"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Generator(\n",
              "  (gen): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): ConvTranspose2d(128, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "    )\n",
              "    (4): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (5): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "step=0\n",
        "critic.train()\n",
        "gen.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "P7xv1l3zfB34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "d849f813-6e9c-4adb-bb60-2c74281ed667"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220808_084646-2l0o3xam</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/saquibali7/uncategorized/runs/2l0o3xam\" target=\"_blank\">distinctive-music-8</a></strong> to <a href=\"https://wandb.ai/saquibali7/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.init(\"task-01\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  for batch_idx, (real) in enumerate(loader):\n",
        "    real = real.to(device)\n",
        "    curr_batch_size = real.shape[0]\n",
        "\n",
        "    for _ in range(CRITIC_ITERATIONS):\n",
        "      noise = torch.randn(curr_batch_size, NOISE_DIM, 1, 1).to(device)\n",
        "      fake = gen(noise)\n",
        "      critic_real = critic(real).reshape(-1)\n",
        "      critic_fake = critic(fake).reshape(-1)\n",
        "      gp = gradient_penalty(critic, real, fake, device=device)\n",
        "      critic_loss = (-(torch.mean(critic_real) - torch.mean(critic_fake)) + 10 * gp)\n",
        "      critic.zero_grad()\n",
        "      critic_loss.backward(retain_graph=True)\n",
        "      opt_critic.step()  \n",
        "\n",
        "    ## Training of Generator\n",
        "\n",
        "    gen_fake = critic(fake).reshape(-1)\n",
        "    loss_gen = -torch.mean(gen_fake)\n",
        "    gen.zero_grad()\n",
        "    loss_gen.backward()\n",
        "    opt_gen.step()\n",
        "\n",
        "  wandb.log({\"Generartor  Loss\": loss_gen, 'epoch':epoch })\n",
        "  wandb.log({\"Discriminator Loss\": critic_loss, 'epoch':epoch })\n",
        "\n",
        "  with torch.inference_mode():\n",
        "   for batch_idx, real_img in enumerate(loader):\n",
        "     noise = torch.randn(curr_batch_size, NOISE_DIM, 1, 1).to(device)\n",
        "     fake_img = gen(noise)\n",
        "     grid1 = make_grid(real_img)\n",
        "     grid2 = make_grid(fake_img)\n",
        "     grid1 = wandb.Image(grid1, caption=\"Original Image\")\n",
        "     grid2 = wandb.Image(grid2, caption=\"Fake Image\")\n",
        "     wandb.log({\"Original Image \": grid1})\n",
        "     wandb.log({\"Fake Image\": grid2})\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aK77fXKhHL5"
      },
      "outputs": [],
      "source": [
        "## for WGAN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bjk2rurgkl1T"
      },
      "outputs": [],
      "source": [
        "ran  = torch.randn((1,128,1,1)).to(device)\n",
        "x = gen(ran)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "6kdJ9fNjGy5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4JpN5-MG3kf",
        "outputId": "2fd33b7e-4fcb-41cf-e161-1c035cef1e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 64, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.detach()."
      ],
      "metadata": {
        "id": "5KCEWBHWIcuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(x[0].permute(1,2,0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "McB7eCKZG961",
        "outputId": "7f68e388-26ea-4ecc-9e81-4fa6ab8bd824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-7d4839f5b2cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'permute'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mfM7hrWfG_m4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "GAN3_WGAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPpEj+5uGbTYKAtqilUTn5O",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}